{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d9d800e-82b2-4096-9e00-03181f89ec1c",
   "metadata": {},
   "source": [
    "### NLP Disaster Tweets\n",
    "\n",
    "Determine if tweet is about real disaster (output 1 if so) or not (then output 0) - Classification task. Competition link can be found [here](https://www.kaggle.com/competitions/nlp-getting-started/overview)<br>\n",
    "\n",
    "#### Evaluation\n",
    "Use F1 score to evaluate where $$F1 = 2*\\frac{precision*recall}{precision+recall}$$ and precision is defined by $$precision = \\frac{TP}{TP+FP}$$ and recall defined by $$recall = \\frac{TP}{TP+FN}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "466ef7ac-eadb-4afb-8735-d0b77c3ccfac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Firstly lets download dataset:\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "path = Path('nlp-getting-started')\n",
    "\n",
    "iskaggle = os.environ.get('KAGGLE_KERNEL_RUN_TYPE', '')\n",
    "if iskaggle:\n",
    "    # if on kaggle we assume we already have data in the directory\n",
    "    !pip install -Uqq fastai # TODO if on a kaggle notebook (this won't work when we go offline though :/)\n",
    "elif not os.path.exists(path):\n",
    "    import zipfile,kaggle\n",
    "    !kaggle competitions download -c nlp-getting-started\n",
    "    zipfile.ZipFile(f'{path}.zip').extractall(path)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0ca93d9-941d-4886-84e5-1bfd61794d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -Uqq fastai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8248e4db-322f-4b48-be5a-43d5d81af47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import stuff you need:\n",
    "from fastai.imports import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f7183e9-9771-4f9b-a060-4ff10d16c410",
   "metadata": {},
   "source": [
    "#### EDA\n",
    "lets figure some things out about our data to see what kind of validation set we ought to build and what features may be important to us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "916e5d38-d288-4f74-9739-101d171e3e2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#3) [Path('nlp-getting-started/sample_submission.csv'),Path('nlp-getting-started/train.csv'),Path('nlp-getting-started/test.csv')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if iskaggle: path = Path('../input/us-patent-phrase-to-phrase-matching')\n",
    "path.ls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b10fc974-d55d-4499-be9b-d4da227122f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are being notified by officers. No other evacuation or shelter in place orders are expected</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation orders in California</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as smoke from #wildfires pours into a school</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7608</th>\n",
       "      <td>10869</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Two giant cranes holding a bridge collapse into nearby homes http://t.co/STfMbbZFB5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7609</th>\n",
       "      <td>10870</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@aria_ahrary @TheTawniest The out of control wild fires in California even in the Northern part of the state. Very troubling.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7610</th>\n",
       "      <td>10871</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M1.94 [01:04 UTC]?5km S of Volcano Hawaii. http://t.co/zDtoyd8EbJ</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7611</th>\n",
       "      <td>10872</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Police investigating after an e-bike collided with a car in Little Portugal. E-bike rider suffered serious non-life threatening injuries.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7612</th>\n",
       "      <td>10873</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The Latest: More Homes Razed by Northern California Wildfire - ABC News http://t.co/YmY4rSkQ3d</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7613 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id keyword location  \\\n",
       "0         1     NaN      NaN   \n",
       "1         4     NaN      NaN   \n",
       "2         5     NaN      NaN   \n",
       "3         6     NaN      NaN   \n",
       "4         7     NaN      NaN   \n",
       "...     ...     ...      ...   \n",
       "7608  10869     NaN      NaN   \n",
       "7609  10870     NaN      NaN   \n",
       "7610  10871     NaN      NaN   \n",
       "7611  10872     NaN      NaN   \n",
       "7612  10873     NaN      NaN   \n",
       "\n",
       "                                                                                                                                           text  \\\n",
       "0                                                                         Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all   \n",
       "1                                                                                                        Forest fire near La Ronge Sask. Canada   \n",
       "2         All residents asked to 'shelter in place' are being notified by officers. No other evacuation or shelter in place orders are expected   \n",
       "3                                                                             13,000 people receive #wildfires evacuation orders in California    \n",
       "4                                                      Just got sent this photo from Ruby #Alaska as smoke from #wildfires pours into a school    \n",
       "...                                                                                                                                         ...   \n",
       "7608                                                        Two giant cranes holding a bridge collapse into nearby homes http://t.co/STfMbbZFB5   \n",
       "7609              @aria_ahrary @TheTawniest The out of control wild fires in California even in the Northern part of the state. Very troubling.   \n",
       "7610                                                                          M1.94 [01:04 UTC]?5km S of Volcano Hawaii. http://t.co/zDtoyd8EbJ   \n",
       "7611  Police investigating after an e-bike collided with a car in Little Portugal. E-bike rider suffered serious non-life threatening injuries.   \n",
       "7612                                             The Latest: More Homes Razed by Northern California Wildfire - ABC News http://t.co/YmY4rSkQ3d   \n",
       "\n",
       "      target  \n",
       "0          1  \n",
       "1          1  \n",
       "2          1  \n",
       "3          1  \n",
       "4          1  \n",
       "...      ...  \n",
       "7608       1  \n",
       "7609       1  \n",
       "7610       1  \n",
       "7611       1  \n",
       "7612       1  \n",
       "\n",
       "[7613 rows x 5 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(path/'train.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d560f651-7ec4-4577-84f0-07449a978260",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3263\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just happened a terrible car crash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Heard about #earthquake is different cities, stay safe everyone.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>there is a forest fire at spot pond, geese are fleeing across the street, I cannot save them all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Apocalypse lighting. #Spokane #wildfires</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3258</th>\n",
       "      <td>10861</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>EARTHQUAKE SAFETY LOS ANGELES ÛÒ SAFETY FASTENERS XrWn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3259</th>\n",
       "      <td>10865</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Storm in RI worse than last hurricane. My city&amp;amp;3others hardest hit. My yard looks like it was bombed. Around 20000K still without power</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3260</th>\n",
       "      <td>10868</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Green Line derailment in Chicago http://t.co/UtbXLcBIuY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3261</th>\n",
       "      <td>10874</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MEG issues Hazardous Weather Outlook (HWO) http://t.co/3X6RBQJHn3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3262</th>\n",
       "      <td>10875</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#CityofCalgary has activated its Municipal Emergency Plan. #yycstorm</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3263 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id keyword location  \\\n",
       "0         0     NaN      NaN   \n",
       "1         2     NaN      NaN   \n",
       "2         3     NaN      NaN   \n",
       "3         9     NaN      NaN   \n",
       "4        11     NaN      NaN   \n",
       "...     ...     ...      ...   \n",
       "3258  10861     NaN      NaN   \n",
       "3259  10865     NaN      NaN   \n",
       "3260  10868     NaN      NaN   \n",
       "3261  10874     NaN      NaN   \n",
       "3262  10875     NaN      NaN   \n",
       "\n",
       "                                                                                                                                             text  \n",
       "0                                                                                                              Just happened a terrible car crash  \n",
       "1                                                                                Heard about #earthquake is different cities, stay safe everyone.  \n",
       "2                                                there is a forest fire at spot pond, geese are fleeing across the street, I cannot save them all  \n",
       "3                                                                                                        Apocalypse lighting. #Spokane #wildfires  \n",
       "4                                                                                                   Typhoon Soudelor kills 28 in China and Taiwan  \n",
       "...                                                                                                                                           ...  \n",
       "3258                                                                                      EARTHQUAKE SAFETY LOS ANGELES ÛÒ SAFETY FASTENERS XrWn  \n",
       "3259  Storm in RI worse than last hurricane. My city&amp;3others hardest hit. My yard looks like it was bombed. Around 20000K still without power  \n",
       "3260                                                                                      Green Line derailment in Chicago http://t.co/UtbXLcBIuY  \n",
       "3261                                                                            MEG issues Hazardous Weather Outlook (HWO) http://t.co/3X6RBQJHn3  \n",
       "3262                                                                         #CityofCalgary has activated its Municipal Emergency Plan. #yycstorm  \n",
       "\n",
       "[3263 rows x 4 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Looking at the tweets, just off intuition I would think that words with hashtags # or @ symbols should be treated\n",
    "# slightly differently, maybe we can incorporate this thinking while we iterate on our model\n",
    "# I also see some strange characters that may not be useful in the data and may just throw the model off (i.e. M1.94, \n",
    "# MontrÌ©al, QuÌ©bec, ÌÏT)\n",
    "eval_df = pd.read_csv(path/'test.csv')\n",
    "print(len(eval_df))\n",
    "eval_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "23d9ef26-0f22-4c75-a2d2-70b2dee5f882",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "keyword\n",
       "fatalities               45\n",
       "deluge                   42\n",
       "armageddon               42\n",
       "sinking                  41\n",
       "damage                   41\n",
       "                         ..\n",
       "forest%20fire            19\n",
       "epicentre                12\n",
       "threat                   11\n",
       "inundation               10\n",
       "radiation%20emergency     9\n",
       "Name: count, Length: 221, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.keyword.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f987dfbd-a68d-4757-8762-cdb6898033ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "location\n",
       "USA                    104\n",
       "New York                71\n",
       "United States           50\n",
       "London                  45\n",
       "Canada                  29\n",
       "                      ... \n",
       "MontrÌ©al, QuÌ©bec       1\n",
       "Montreal                 1\n",
       "ÌÏT: 6.4682,3.18287      1\n",
       "Live4Heed??              1\n",
       "Lincoln                  1\n",
       "Name: count, Length: 3341, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.location.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "901f4656-98e7-4b10-b95c-07b9128f11ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text\n",
       "11-Year-Old Boy Charged With Manslaughter of Toddler: Report: An 11-year-old boy has been charged with manslaughter over the fatal sh...        10\n",
       "He came to a land which was engulfed in tribal war and turned it into a land of peace i.e. Madinah. #ProphetMuhammad #islam                      6\n",
       "The Prophet (peace be upon him) said 'Save yourself from Hellfire even if it is by giving half a date in charity.'                               6\n",
       "#Bestnaijamade: 16yr old PKK suicide bomber who detonated bomb in ... http://t.co/KSAwlYuX02 bestnaijamade bestnaijamade bestnaijamade beÛ_     6\n",
       "Madhya Pradesh Train Derailment: Village Youth Saved Many Lives                                                                                  5\n",
       "                                                                                                                                                ..\n",
       "Escape The Heat (and the #ORShow) for a trail run on Desolation Loop you'll be glad you did http://t.co/n2ucNzh38P http://t.co/VU8fWYMw5r        1\n",
       "Hey girl you must be Toe Hobbit: Part Two: ghe Desolation of Smaug because I'm not interested in seeing you. Sorry.                              1\n",
       "Emotional Desolation the effect of alcoholism/addiction on family - http://t.co/31tGtLz3YA Forgiving is hard http://t.co/C7rcO2eMwF              1\n",
       "Fotoset: elanorofrohan: 10th December 2013 Green Carpet in Zurich for the Swiss Premiere of The Desolation... http://t.co/BQ3P7n7w06             1\n",
       "The Latest: More Homes Razed by Northern California Wildfire - ABC News http://t.co/YmY4rSkQ3d                                                   1\n",
       "Name: count, Length: 7503, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We see that some texts are actually repeated in the dataset, definitely don't want some to be in training, some in \n",
    "# validation so we should make sure all repeated texts are in one or the other (no duplicates) so that it gives us a better\n",
    "# idea as to whether or not our model is actually doing better\n",
    "df.text.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4a141b80-a2da-47e5-8661-e79a7c5741d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# This is interesting as based off first and last 5 entries of df I assumed 1 was much more common\n",
    "# May want to shuffle all of the indices around\n",
    "df.target.hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18369964-ba8f-43ec-8cbc-fae62bb78bab",
   "metadata": {},
   "source": [
    "### Training setup\n",
    "We will use transformers library + smaller pretrained model so we can do a bunch of iterations to see what changes are the best at affecting our metric (F1 score). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "efd9b691-6869-49d1-bb9a-b17cd1b687d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings,transformers,logging,torch\n",
    "from transformers import TrainingArguments,Trainer\n",
    "from transformers import AutoModelForSequenceClassification,AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e142ae0a-547c-4ecc-a7ba-6c73afd99761",
   "metadata": {},
   "outputs": [],
   "source": [
    "if iskaggle:\n",
    "    !pip install -q datasets\n",
    "import datasets\n",
    "from datasets import load_dataset, Dataset, DatasetDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4cc6768b-8ef2-46aa-8fbf-cc4945bacc74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this will quiet a lot of hugging face warnings and verbosity\n",
    "warnings.simplefilter('ignore')\n",
    "logging.disable(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "36f314da-2a61-42a4-9687-5179e9bb4427",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a small simple model \n",
    "model_nm = 'distilbert/distilbert-base-uncased'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d8f10e4d-65b1-4b74-8b35-2cd95cbf52d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use autotokenizer to look at its vocab and see how\n",
    "tokz = AutoTokenizer.from_pretrained(model_nm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "51ed11cb-84e3-4dc7-abab-578636e42b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to come up with a way to combine keyword, location, and text, for now can just do this with separator token\n",
    "sep = tokz.sep_token\n",
    "df['inputs'] = f'keyword: {df.keyword}{sep} location: {df.location}{sep} text: {df.text}'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "97e746ca-2af8-49c4-ba42-22fe50bf0201",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = Dataset.from_pandas(df).rename_column('target', 'label') # df -> hf dataset, and rename to label so we can use for training\n",
    "eval_ds = Dataset.from_pandas(eval_df)\n",
    "\n",
    "def tokenize(x): return tokz(x['inputs'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4abdbc69-c460-43ad-8b0c-ed43bceb4680",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'input_ids': [101, 3145, 18351, 1024, 1014, 16660, 1015, 16660, 1016, 16660, 1017, 16660, 1018, 16660, 1012, 1012, 1012, 24643, 2620, 16660, 24643, 2683, 16660, 6146, 10790, 16660, 6146, 14526, 16660, 6146, 12521, 16660, 2171, 1024, 3145, 18351, 1010, 3091, 1024, 6146, 17134, 1010, 26718, 18863, 1024, 4874, 102, 3295, 1024, 1014, 16660, 1015, 16660, 1016, 16660, 1017, 16660, 1018, 16660, 1012, 1012, 1012, 24643, 2620, 16660, 24643, 2683, 16660, 6146, 10790, 16660, 6146, 14526, 16660, 6146, 12521, 16660, 2171, 1024, 3295, 1010, 3091, 1024, 6146, 17134, 1010, 26718, 18863, 1024, 4874, 102, 3793, 1024, 1014, 2256, 15616, 2024, 1996, 3114, 1997, 2023, 1001, 8372, 2089, 16455, 9641, 2149, 2035, 1015, 3224, 2543, 2379, 2474, 6902, 3351, 21871, 2243, 1012, 2710, 1016, 2035, 3901, 2356, 2000, 1005, 7713, 1999, 2173, 1005, 2024, 2108, 19488, 2011, 3738, 1012, 2053, 2060, 13982, 2030, 7713, 1999, 2173, 4449, 2024, 3517, 1017, 2410, 1010, 2199, 2111, 4374, 1001, 3748, 26332, 13982, 4449, 1999, 2662, 1018, 2074, 2288, 2741, 2023, 6302, 2013, 10090, 1001, 7397, 2004, 5610, 2013, 1001, 3748, 26332, 10364, 2015, 2046, 1037, 2082, 1012, 1012, 1012, 24643, 2620, 2048, 5016, 27083, 3173, 1037, 2958, 7859, 2046, 3518, 5014, 8299, 1024, 1013, 1013, 1056, 1012, 2522, 1013, 2358, 16715, 10322, 2480, 26337, 2629, 24643, 2683, 1030, 9342, 1035, 6289, 19848, 2100, 1030, 23963, 7962, 10458, 1996, 2041, 1997, 2491, 3748, 8769, 1999, 2662, 2130, 1999, 1996, 2642, 2112, 1997, 1996, 2110, 1012, 2200, 19817, 7140, 9709, 1012, 6146, 10790, 23290, 1012, 6365, 1031, 5890, 1024, 5840, 11396, 1033, 1029, 1019, 22287, 1055, 1997, 12779, 7359, 1012, 8299, 1024, 1013, 1013, 1056, 1012, 2522, 1013, 1062, 11927, 6977, 2094, 2620, 15878, 3501, 6146, 14526, 2610, 11538, 2044, 2019, 1041, 1011, 7997, 17745, 2007, 1037, 2482, 1999, 2210, 5978, 1012, 1041, 1011, 7997, 7945, 4265, 3809, 2512, 1011, 2166, 8701, 6441, 1012, 6146, 12521, 1996, 6745, 1024, 2062, 5014, 10958, 5422, 2011, 2642, 2662, 3748, 10273, 1011, 5925, 2739, 8299, 1024, 1013, 1013, 1056, 1012, 2522, 1013, 1061, 8029, 2549, 27472, 4160, 29097, 2171, 1024, 3793, 1010, 3091, 1024, 6146, 17134, 1010, 26718, 18863, 1024, 4874, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]},\n",
       " {'id': 1,\n",
       "  'keyword': None,\n",
       "  'location': None,\n",
       "  'text': 'Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all',\n",
       "  'label': 1,\n",
       "  'inputs': \"keyword: 0       NaN\\n1       NaN\\n2       NaN\\n3       NaN\\n4       NaN\\n       ... \\n7608    NaN\\n7609    NaN\\n7610    NaN\\n7611    NaN\\n7612    NaN\\nName: keyword, Length: 7613, dtype: object[SEP] location: 0       NaN\\n1       NaN\\n2       NaN\\n3       NaN\\n4       NaN\\n       ... \\n7608    NaN\\n7609    NaN\\n7610    NaN\\n7611    NaN\\n7612    NaN\\nName: location, Length: 7613, dtype: object[SEP] text: 0                                                                           Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all\\n1                                                                                                          Forest fire near La Ronge Sask. Canada\\n2           All residents asked to 'shelter in place' are being notified by officers. No other evacuation or shelter in place orders are expected\\n3                                                                               13,000 people receive #wildfires evacuation orders in California \\n4                                                        Just got sent this photo from Ruby #Alaska as smoke from #wildfires pours into a school \\n                                                                          ...                                                                    \\n7608                                                          Two giant cranes holding a bridge collapse into nearby homes http://t.co/STfMbbZFB5\\n7609                @aria_ahrary @TheTawniest The out of control wild fires in California even in the Northern part of the state. Very troubling.\\n7610                                                                            M1.94 [01:04 UTC]?5km S of Volcano Hawaii. http://t.co/zDtoyd8EbJ\\n7611    Police investigating after an e-bike collided with a car in Little Portugal. E-bike rider suffered serious non-life threatening injuries.\\n7612                                               The Latest: More Homes Razed by Northern California Wildfire - ABC News http://t.co/YmY4rSkQ3d\\nName: text, Length: 7613, dtype: object\"})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize(ds[0]), ds[0] # tokenizer seems to be working well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4e20f68e-6466-47e3-81e6-94bbaf3407d1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32ffb5079ee743ce850d47fdd0bd346c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7613 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'label': 1,\n",
       " 'input_ids': [101,\n",
       "  3145,\n",
       "  18351,\n",
       "  1024,\n",
       "  1014,\n",
       "  16660,\n",
       "  1015,\n",
       "  16660,\n",
       "  1016,\n",
       "  16660,\n",
       "  1017,\n",
       "  16660,\n",
       "  1018,\n",
       "  16660,\n",
       "  1012,\n",
       "  1012,\n",
       "  1012,\n",
       "  24643,\n",
       "  2620,\n",
       "  16660,\n",
       "  24643,\n",
       "  2683,\n",
       "  16660,\n",
       "  6146,\n",
       "  10790,\n",
       "  16660,\n",
       "  6146,\n",
       "  14526,\n",
       "  16660,\n",
       "  6146,\n",
       "  12521,\n",
       "  16660,\n",
       "  2171,\n",
       "  1024,\n",
       "  3145,\n",
       "  18351,\n",
       "  1010,\n",
       "  3091,\n",
       "  1024,\n",
       "  6146,\n",
       "  17134,\n",
       "  1010,\n",
       "  26718,\n",
       "  18863,\n",
       "  1024,\n",
       "  4874,\n",
       "  102,\n",
       "  3295,\n",
       "  1024,\n",
       "  1014,\n",
       "  16660,\n",
       "  1015,\n",
       "  16660,\n",
       "  1016,\n",
       "  16660,\n",
       "  1017,\n",
       "  16660,\n",
       "  1018,\n",
       "  16660,\n",
       "  1012,\n",
       "  1012,\n",
       "  1012,\n",
       "  24643,\n",
       "  2620,\n",
       "  16660,\n",
       "  24643,\n",
       "  2683,\n",
       "  16660,\n",
       "  6146,\n",
       "  10790,\n",
       "  16660,\n",
       "  6146,\n",
       "  14526,\n",
       "  16660,\n",
       "  6146,\n",
       "  12521,\n",
       "  16660,\n",
       "  2171,\n",
       "  1024,\n",
       "  3295,\n",
       "  1010,\n",
       "  3091,\n",
       "  1024,\n",
       "  6146,\n",
       "  17134,\n",
       "  1010,\n",
       "  26718,\n",
       "  18863,\n",
       "  1024,\n",
       "  4874,\n",
       "  102,\n",
       "  3793,\n",
       "  1024,\n",
       "  1014,\n",
       "  2256,\n",
       "  15616,\n",
       "  2024,\n",
       "  1996,\n",
       "  3114,\n",
       "  1997,\n",
       "  2023,\n",
       "  1001,\n",
       "  8372,\n",
       "  2089,\n",
       "  16455,\n",
       "  9641,\n",
       "  2149,\n",
       "  2035,\n",
       "  1015,\n",
       "  3224,\n",
       "  2543,\n",
       "  2379,\n",
       "  2474,\n",
       "  6902,\n",
       "  3351,\n",
       "  21871,\n",
       "  2243,\n",
       "  1012,\n",
       "  2710,\n",
       "  1016,\n",
       "  2035,\n",
       "  3901,\n",
       "  2356,\n",
       "  2000,\n",
       "  1005,\n",
       "  7713,\n",
       "  1999,\n",
       "  2173,\n",
       "  1005,\n",
       "  2024,\n",
       "  2108,\n",
       "  19488,\n",
       "  2011,\n",
       "  3738,\n",
       "  1012,\n",
       "  2053,\n",
       "  2060,\n",
       "  13982,\n",
       "  2030,\n",
       "  7713,\n",
       "  1999,\n",
       "  2173,\n",
       "  4449,\n",
       "  2024,\n",
       "  3517,\n",
       "  1017,\n",
       "  2410,\n",
       "  1010,\n",
       "  2199,\n",
       "  2111,\n",
       "  4374,\n",
       "  1001,\n",
       "  3748,\n",
       "  26332,\n",
       "  13982,\n",
       "  4449,\n",
       "  1999,\n",
       "  2662,\n",
       "  1018,\n",
       "  2074,\n",
       "  2288,\n",
       "  2741,\n",
       "  2023,\n",
       "  6302,\n",
       "  2013,\n",
       "  10090,\n",
       "  1001,\n",
       "  7397,\n",
       "  2004,\n",
       "  5610,\n",
       "  2013,\n",
       "  1001,\n",
       "  3748,\n",
       "  26332,\n",
       "  10364,\n",
       "  2015,\n",
       "  2046,\n",
       "  1037,\n",
       "  2082,\n",
       "  1012,\n",
       "  1012,\n",
       "  1012,\n",
       "  24643,\n",
       "  2620,\n",
       "  2048,\n",
       "  5016,\n",
       "  27083,\n",
       "  3173,\n",
       "  1037,\n",
       "  2958,\n",
       "  7859,\n",
       "  2046,\n",
       "  3518,\n",
       "  5014,\n",
       "  8299,\n",
       "  1024,\n",
       "  1013,\n",
       "  1013,\n",
       "  1056,\n",
       "  1012,\n",
       "  2522,\n",
       "  1013,\n",
       "  2358,\n",
       "  16715,\n",
       "  10322,\n",
       "  2480,\n",
       "  26337,\n",
       "  2629,\n",
       "  24643,\n",
       "  2683,\n",
       "  1030,\n",
       "  9342,\n",
       "  1035,\n",
       "  6289,\n",
       "  19848,\n",
       "  2100,\n",
       "  1030,\n",
       "  23963,\n",
       "  7962,\n",
       "  10458,\n",
       "  1996,\n",
       "  2041,\n",
       "  1997,\n",
       "  2491,\n",
       "  3748,\n",
       "  8769,\n",
       "  1999,\n",
       "  2662,\n",
       "  2130,\n",
       "  1999,\n",
       "  1996,\n",
       "  2642,\n",
       "  2112,\n",
       "  1997,\n",
       "  1996,\n",
       "  2110,\n",
       "  1012,\n",
       "  2200,\n",
       "  19817,\n",
       "  7140,\n",
       "  9709,\n",
       "  1012,\n",
       "  6146,\n",
       "  10790,\n",
       "  23290,\n",
       "  1012,\n",
       "  6365,\n",
       "  1031,\n",
       "  5890,\n",
       "  1024,\n",
       "  5840,\n",
       "  11396,\n",
       "  1033,\n",
       "  1029,\n",
       "  1019,\n",
       "  22287,\n",
       "  1055,\n",
       "  1997,\n",
       "  12779,\n",
       "  7359,\n",
       "  1012,\n",
       "  8299,\n",
       "  1024,\n",
       "  1013,\n",
       "  1013,\n",
       "  1056,\n",
       "  1012,\n",
       "  2522,\n",
       "  1013,\n",
       "  1062,\n",
       "  11927,\n",
       "  6977,\n",
       "  2094,\n",
       "  2620,\n",
       "  15878,\n",
       "  3501,\n",
       "  6146,\n",
       "  14526,\n",
       "  2610,\n",
       "  11538,\n",
       "  2044,\n",
       "  2019,\n",
       "  1041,\n",
       "  1011,\n",
       "  7997,\n",
       "  17745,\n",
       "  2007,\n",
       "  1037,\n",
       "  2482,\n",
       "  1999,\n",
       "  2210,\n",
       "  5978,\n",
       "  1012,\n",
       "  1041,\n",
       "  1011,\n",
       "  7997,\n",
       "  7945,\n",
       "  4265,\n",
       "  3809,\n",
       "  2512,\n",
       "  1011,\n",
       "  2166,\n",
       "  8701,\n",
       "  6441,\n",
       "  1012,\n",
       "  6146,\n",
       "  12521,\n",
       "  1996,\n",
       "  6745,\n",
       "  1024,\n",
       "  2062,\n",
       "  5014,\n",
       "  10958,\n",
       "  5422,\n",
       "  2011,\n",
       "  2642,\n",
       "  2662,\n",
       "  3748,\n",
       "  10273,\n",
       "  1011,\n",
       "  5925,\n",
       "  2739,\n",
       "  8299,\n",
       "  1024,\n",
       "  1013,\n",
       "  1013,\n",
       "  1056,\n",
       "  1012,\n",
       "  2522,\n",
       "  1013,\n",
       "  1061,\n",
       "  8029,\n",
       "  2549,\n",
       "  27472,\n",
       "  4160,\n",
       "  29097,\n",
       "  2171,\n",
       "  1024,\n",
       "  3793,\n",
       "  1010,\n",
       "  3091,\n",
       "  1024,\n",
       "  6146,\n",
       "  17134,\n",
       "  1010,\n",
       "  26718,\n",
       "  18863,\n",
       "  1024,\n",
       "  4874,\n",
       "  102],\n",
       " 'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1]}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take out all of these extra attributes as no longer needed, all data was put into inputs, inputs was tokenized, tokens are all\n",
    "# the input data we need for NLP model\n",
    "inps = 'keyword', 'location', 'text', 'id', 'inputs' \n",
    "tok_ds = ds.map(tokenize, batched=True, remove_columns=inps)\n",
    "\n",
    "tok_ds[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c85b0b60-ca9b-468b-8cf4-426e66266be0",
   "metadata": {},
   "source": [
    "### Validation set\n",
    "\n",
    "Seems like the best thing to do is probably just get a sample for validation set, maybe not including the same text? Or maybe with to incorporate recall which is a part of our metric. For now we go for easiest and simplest way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "617184a3-a555-48e9-a4d8-4c1cf73a5908",
   "metadata": {},
   "outputs": [],
   "source": [
    "dds = tok_ds.train_test_split(test_size=.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850816b6-b18d-42e7-9cab-20486c7d6b2e",
   "metadata": {},
   "source": [
    "### Initial Model\n",
    "Need to provide a metric, namely the F1 score. Firstly we will define a function that can compute it given preds and true. Then we will create a function that uses this F1 method and passes back a dict so Transformers library can use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1d45934e-09f3-4186-aa48-228c76b4f45d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5859636093575937"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compute_f1(preds, true):\n",
    "    preds, true = np.array(preds), np.array(true)\n",
    "    # ensure they have the same shape\n",
    "    if preds.shape!=true.shape:\n",
    "        print(f'shapes: preds: {preds.shape} true: {true.shape}')\n",
    "\n",
    "    # compute true positives\n",
    "    TP = np.sum((preds==1) & (true==1))\n",
    "    # compute false positives\n",
    "    FP = np.sum((preds==1) & (true==0))\n",
    "    # compute false negatives\n",
    "    FN = np.sum((preds==0) & (true==1))\n",
    "\n",
    "    precision = TP/(TP+FP)\n",
    "    recall = TP/(TP+FN)\n",
    "\n",
    "    return (2*precision*recall)/(precision+recall) # f1 formula\n",
    "\n",
    "bs_preds = np.ones_like(dds['test']['label'])\n",
    "compute_f1(bs_preds, dds['test']['label']) # Seems to work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2c7a57d3-d210-4154-9e17-6a357cbf8418",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1(eval_preds): return {'F1':compute_f1(*eval_preds) } # This is the function we will pass to transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "277beb65-2404-42e6-b3be-47c791fd0dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick reasonable lr, wd, bs, epochs\n",
    "lr,bs = 8e-5,32\n",
    "wd,epochs = 0.01,3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ef79af46-09f3-4cb8-918c-d66fcc1ce8cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function from fastai grandmaster notebook, look there for more expo\n",
    "def get_trainer(dds):\n",
    "    args = TrainingArguments('outputs', learning_rate=lr, warmup_ratio=0.1, lr_scheduler_type='cosine', fp16=True,\n",
    "        evaluation_strategy=\"epoch\", per_device_train_batch_size=bs, per_device_eval_batch_size=bs*2,\n",
    "        num_train_epochs=epochs, weight_decay=wd, report_to='none')\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_nm, num_labels=1)\n",
    "    return Trainer(model, args, train_dataset=dds['train'], eval_dataset=dds['test'],\n",
    "                   tokenizer=tokz, compute_metrics=f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a460eada-507c-45a5-9460-13f55e362944",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainer = get_trainer(dds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "09cf266c-dbd1-4b56-a66a-b2aa4bfad873",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Found dtype Long but expected Float",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# The reason we are encountering this error, according to stackoverflow, is because our problem is a single label\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# is a single label binary classification problem so we need to use a different loss function\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Look at the below link to see that when num_labels==1 its seen as regression, > 1 its thought of as classification\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# https://github.com/huggingface/transformers/blob/05fa1a7ac17bb7aa07b9e0c1e138ecb31a28bbfe/src/transformers/models/bert/modeling_bert.py#L1563\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m;\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/transformers/trainer.py:1885\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1883\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1884\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1885\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/transformers/trainer.py:2216\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2213\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m   2215\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 2216\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2218\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2219\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2220\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2221\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2222\u001b[0m ):\n\u001b[1;32m   2223\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2224\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/transformers/trainer.py:3250\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   3248\u001b[0m         scaled_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m   3249\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3250\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3252\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach() \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/accelerate/accelerator.py:2155\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   2153\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m   2154\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 2155\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2156\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m learning_rate \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhas_lomo_optimizer:\n\u001b[1;32m   2157\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlomo_backward(loss, learning_rate)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    520\u001b[0m     )\n\u001b[0;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/autograd/__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/autograd/graph.py:768\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    766\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    767\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 768\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    769\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    770\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    771\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    772\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Found dtype Long but expected Float"
     ]
    }
   ],
   "source": [
    "# The reason we are encountering this error, according to stackoverflow, is because our problem is a single label\n",
    "# is a single label binary classification problem so we need to use a different loss function\n",
    "# Look at the below link to see that when num_labels==1 its seen as regression, > 1 its thought of as classification\n",
    "# https://github.com/huggingface/transformers/blob/05fa1a7ac17bb7aa07b9e0c1e138ecb31a28bbfe/src/transformers/models/bert/modeling_bert.py#L1563\n",
    "trainer.train();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d394c2f4-79ec-4886-8b24-61277c600922",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All we are going to change is num_labels lets see what happens:\n",
    "def get_trainer(dds):\n",
    "    args = TrainingArguments('outputs', learning_rate=lr, warmup_ratio=0.1, lr_scheduler_type='cosine', fp16=True,\n",
    "        evaluation_strategy=\"epoch\", per_device_train_batch_size=bs, per_device_eval_batch_size=bs*2,\n",
    "        num_train_epochs=epochs, weight_decay=wd, report_to='none')\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_nm, num_labels=2)\n",
    "    return Trainer(model, args, train_dataset=dds['train'], eval_dataset=dds['test'],\n",
    "                   tokenizer=tokz, compute_metrics=f1)\n",
    "\n",
    "trainer=get_trainer(dds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "96ae1727-5fd4-4adf-8ed2-0c124dea2c2b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='46' max='135' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 46/135 00:22 < 00:46, 1.91 it/s, Epoch 1/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9' max='8' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [8/8 00:34]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shapes: preds: (1904, 2) true: (1904,)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (1904,2) (1904,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# getting this error because we don't understand EvalPrediction object transformers returns to help us create metrics for \u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# training and inference. \u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m;\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/transformers/trainer.py:1885\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1883\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1884\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1885\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/transformers/trainer.py:2311\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2308\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_training_stop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   2310\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_epoch_end(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[0;32m-> 2311\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_log_save_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_norm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2313\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m DebugOption\u001b[38;5;241m.\u001b[39mTPU_METRICS_DEBUG \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdebug:\n\u001b[1;32m   2314\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_torch_xla_available():\n\u001b[1;32m   2315\u001b[0m         \u001b[38;5;66;03m# tpu-comment: Logging debug metrics for PyTorch/XLA (compile, execute times, ops, etc.)\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/transformers/trainer.py:2721\u001b[0m, in \u001b[0;36mTrainer._maybe_log_save_evaluate\u001b[0;34m(self, tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2719\u001b[0m metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2720\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_evaluate:\n\u001b[0;32m-> 2721\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2722\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_report_to_hp_search(trial, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step, metrics)\n\u001b[1;32m   2724\u001b[0m     \u001b[38;5;66;03m# Run delayed LR scheduler now that metrics are populated\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/transformers/trainer.py:3572\u001b[0m, in \u001b[0;36mTrainer.evaluate\u001b[0;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   3569\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m   3571\u001b[0m eval_loop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprediction_loop \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39muse_legacy_prediction_loop \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluation_loop\n\u001b[0;32m-> 3572\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43meval_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3573\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3574\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdescription\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEvaluation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3575\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# No point gathering the predictions if there are no metrics, otherwise we defer to\u001b[39;49;00m\n\u001b[1;32m   3576\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# self.args.prediction_loss_only\u001b[39;49;00m\n\u001b[1;32m   3577\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprediction_loss_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   3578\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3579\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3580\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3582\u001b[0m total_batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39meval_batch_size \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mworld_size\n\u001b[1;32m   3583\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric_key_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_jit_compilation_time\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m output\u001b[38;5;241m.\u001b[39mmetrics:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/transformers/trainer.py:3854\u001b[0m, in \u001b[0;36mTrainer.evaluation_loop\u001b[0;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   3850\u001b[0m         metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_metrics(\n\u001b[1;32m   3851\u001b[0m             EvalPrediction(predictions\u001b[38;5;241m=\u001b[39mall_preds, label_ids\u001b[38;5;241m=\u001b[39mall_labels, inputs\u001b[38;5;241m=\u001b[39mall_inputs)\n\u001b[1;32m   3852\u001b[0m         )\n\u001b[1;32m   3853\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3854\u001b[0m         metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mEvalPrediction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredictions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mall_preds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mall_labels\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3855\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m metrics \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3856\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m {}\n",
      "Cell \u001b[0;32mIn[22], line 1\u001b[0m, in \u001b[0;36mf1\u001b[0;34m(eval_preds)\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mf1\u001b[39m(eval_preds): \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mF1\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[43mcompute_f1\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43meval_preds\u001b[49m\u001b[43m)\u001b[49m }\n",
      "Cell \u001b[0;32mIn[21], line 8\u001b[0m, in \u001b[0;36mcompute_f1\u001b[0;34m(preds, true)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mshapes: preds: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpreds\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m true: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrue\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# compute true positives\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m TP \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(\u001b[43m(\u001b[49m\u001b[43mpreds\u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m&\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrue\u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# compute false positives\u001b[39;00m\n\u001b[1;32m     10\u001b[0m FP \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum((preds\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m&\u001b[39m (true\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m0\u001b[39m))\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (1904,2) (1904,) "
     ]
    }
   ],
   "source": [
    "# getting this error because we don't understand EvalPrediction object transformers returns to help us create metrics for \n",
    "# training and inference. \n",
    "trainer.train();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "09daab4e-d897-40ad-8de8-d1d1be843b9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PredictionOutput(predictions=array([[ 0.08377087, -0.07733025],\n",
       "       [ 0.08377087, -0.07733025],\n",
       "       [ 0.08377087, -0.07733025],\n",
       "       [ 0.08377087, -0.07733025],\n",
       "       [ 0.08377087, -0.07733025],\n",
       "       [ 0.08377087, -0.07733025],\n",
       "       [ 0.08377087, -0.07733025],\n",
       "       [ 0.08377087, -0.07733025],\n",
       "       [ 0.08377087, -0.07733025],\n",
       "       [ 0.08377089, -0.07733027]], dtype=float32), label_ids=array([1, 0, 1, 1, 0, 1, 1, 0, 1, 1]), metrics={'test_loss': 0.7366630434989929, 'test_runtime': 0.1396, 'test_samples_per_second': 71.614, 'test_steps_per_second': 7.161})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets print the Eval prediction object we generate during inference and then recreate our F1 score fn\n",
    "# WARNING: we have reinitialized trainer to have no compute_metric\n",
    "trainer.compute_metrics=None\n",
    "eval_pred = trainer.predict(test_dataset=dds['test'].select(range(10))) # run predictions on small subset of validation set\n",
    "eval_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5fec0c2-e2ad-43d7-a9bb-dd00d2b9359f",
   "metadata": {},
   "source": [
    "As you can see the predictions are a 2D array. After doing some research, the numbers for each index (row) of the predictions array represent the raw logits of the model for each class.\n",
    " - If we had more classes (categories), we would have more columns\n",
    " - We feed these logits through a softmax to get the probabilities of the model\n",
    " - To get the actual predictions (i.e whether for example 1 we predict it to be in class 0 or 1) we need to take the argmax along axis 1 (along the columns)\n",
    " - This is because applying softmax doesn't change relative order of the logits/probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4158e4f6-b718-49f1-9f28-f773c8396c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rewriting our F1 function:\n",
    "# Going to use scikit learn function to handle all edge cases for us too\n",
    "from sklearn.metrics import f1_score\n",
    "def compute_f1(preds, true):\n",
    "    preds, true = np.array(preds), np.array(true)\n",
    "    # Applying argmax as we talked about above\n",
    "    preds = np.argmax(preds, axis=1)  \n",
    "    # Use external library \n",
    "    return f1_score(true, preds)\n",
    "\n",
    "def f1(eval_preds): return {'F1':compute_f1(*eval_preds) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "90ca60c1-cba6-42a9-bcb3-31380d181ffa",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='56' max='135' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 56/135 00:31 < 00:45, 1.74 it/s, Epoch 1.22/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.683222</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m trainer \u001b[38;5;241m=\u001b[39m get_trainer(dds)\n\u001b[0;32m----> 3\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m;\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/transformers/trainer.py:1885\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1883\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1884\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1885\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/transformers/trainer.py:2221\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2215\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[1;32m   2216\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs)\n\u001b[1;32m   2218\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2219\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2220\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[0;32m-> 2221\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misinf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss_step\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   2222\u001b[0m ):\n\u001b[1;32m   2223\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2224\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n\u001b[1;32m   2225\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer = get_trainer(dds)\n",
    "\n",
    "trainer.train();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99ca7a3-ecbc-44a1-9d9f-80468ce74007",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets try to troubleshoot why F1 is always 0\n",
    "eval_predictions = trainer.predict(test_dataset=dds['test'].select(range(10)))\n",
    "pred_labels = np.argmax(eval_predictions.predictions, axis=1)\n",
    "print(\"Predicted Labels Distribution:\", np.bincount(pred_labels), np.bincount(eval_predictions.label_ids))\n",
    "print(eval_predictions.predictions)\n",
    "# So it seems that its only predicting 0, likely bc we initialized model with AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07fc7d65-ce8e-41b5-a0bb-5364a38fa151",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets try to just make learning rate smaller and try again, maybe because this model is bigger than deberta small?\n",
    "lr,bs = 8e-7,16\n",
    "wd,epochs = 0.01,3\n",
    "\n",
    "trainer = get_trainer(dds)\n",
    "trainer.train();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d586cb-400c-4cae-bf12-b82a6b4378fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# maybe I don't understand something about BERT? lets try different model\n",
    "model_nm = 'microsoft/deberta-v3-small'\n",
    "\n",
    "trainer = get_trainer(dds)\n",
    "trainer.model.config._name_or_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f672c1aa-111e-404e-a3ed-aa8a78ce5e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "46fa8943-8822-4b6c-8d98-6c3b12be64e6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d51fea2428134ec6833f80712f0d7a86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting the dataset:   0%|          | 0/5709 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e48656ccb8d1413d8ee2c81565c7d8ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting the dataset:   0%|          | 0/1904 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import Value\n",
    "# Trying changing problem type and casting labels to floats\n",
    "def get_trainer(dds):\n",
    "    args = TrainingArguments('outputs', learning_rate=lr, warmup_ratio=0.1, lr_scheduler_type='cosine',\n",
    "                             fp16=True, evaluation_strategy=\"epoch\", per_device_train_batch_size=bs, \n",
    "                             per_device_eval_batch_size=bs*2, num_train_epochs=epochs, weight_decay=wd, report_to='none')\n",
    "    \n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_nm, \n",
    "                                                               num_labels=1,\n",
    "                                                               problem_type = \"single_label_classification\")\n",
    "    return Trainer(model, args, train_dataset=dds['train'], eval_dataset=dds['test'],\n",
    "                   tokenizer=tokz, compute_metrics=f1)\n",
    "\n",
    "new_features = dds['test'].features.copy()\n",
    "new_features['label']=Value(dtype='float32')\n",
    "\n",
    "f_dds = dds.cast(new_features)\n",
    "f_dds['train'].features\n",
    "\n",
    "trainer = get_trainer(dds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "84496b1f-67c0-4c5f-9a7b-ca8adf79656e",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m;\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/transformers/trainer.py:1885\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1883\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1884\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1885\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/transformers/trainer.py:1895\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1892\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_inner_training_loop\u001b[39m(\n\u001b[1;32m   1893\u001b[0m     \u001b[38;5;28mself\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, args\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, resume_from_checkpoint\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, trial\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, ignore_keys_for_eval\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1894\u001b[0m ):\n\u001b[0;32m-> 1895\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfree_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1896\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_batch_size \u001b[38;5;241m=\u001b[39m batch_size\n\u001b[1;32m   1897\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mauto_find_batch_size:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/accelerate/accelerator.py:3195\u001b[0m, in \u001b[0;36mAccelerator.free_memory\u001b[0;34m(self, *objects)\u001b[0m\n\u001b[1;32m   3193\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeepspeed_engine_wrapped\u001b[38;5;241m.\u001b[39mengine\u001b[38;5;241m.\u001b[39mdestroy()\n\u001b[1;32m   3194\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeepspeed_engine_wrapped \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 3195\u001b[0m objects \u001b[38;5;241m=\u001b[39m \u001b[43mrelease_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mobjects\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3196\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_schedulers \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m   3197\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizers \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/accelerate/utils/memory.py:77\u001b[0m, in \u001b[0;36mrelease_memory\u001b[0;34m(*objects)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(objects)):\n\u001b[1;32m     76\u001b[0m     objects[i] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 77\u001b[0m \u001b[43mclear_device_cache\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgarbage_collection\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m objects\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/accelerate/utils/memory.py:48\u001b[0m, in \u001b[0;36mclear_device_cache\u001b[0;34m(garbage_collection)\u001b[0m\n\u001b[1;32m     46\u001b[0m     torch\u001b[38;5;241m.\u001b[39mmps\u001b[38;5;241m.\u001b[39mempty_cache()\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 48\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mempty_cache\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/cuda/memory.py:170\u001b[0m, in \u001b[0;36mempty_cache\u001b[0;34m()\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Release all unoccupied cached memory currently held by the caching\u001b[39;00m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;124;03mallocator so that those can be used in other GPU application and visible in\u001b[39;00m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;124;03m`nvidia-smi`.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;124;03m    more details about GPU memory management.\u001b[39;00m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_initialized():\n\u001b[0;32m--> 170\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cuda_emptyCache\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "trainer.train();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f55a8e5f-6559-4149-8229-3aeb248e517f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
